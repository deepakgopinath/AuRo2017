How can the experiment be redesigned?

What metrics can be looked at?

Rate of mode switches before and after the assistance across trials?, Should be significantly lower in the ideal case. If the mode selected was different. 
Probability of the most probably goal after assistance across trials? Should expect a sharp increase

Any goal? Or fixed goals?

Blending always present. Very high threshold for helping. Should the threshold be adaptive? Sensitive to Time. progress to goal?
No mode switch assistance. Manual mode switch. 
Request assistance - Mandatory request. 
Intermittent assistance. Timer based?

What other metrics. Number of requests? 
Subjective metrics?

How to prime the user?

User needs to know what the blending feels like. Should the user know how the robot gains confidence? (Whether it is directedness or distance base or agreement based?)
Possible experiment idea:

1. Simplify the setup possibly. 3 goals, same orientation. 
								4 goals, different orientation.

Any goal the user wants. However, blending has a much higher threshold. Therefore will only step if really really confident. 

a. Trials with no requested assistance.
b. Trials with request assistance. (mandatory?). At least one. How to correlate request with "progress towards goal (or the lack thereof)"
   or
   Trials with automated assistance. Assistance every 10-15 seconds? Automatically triggered. Or 
possibly normalize the trajectory length. 

hat do we expect?
If the assistance makes any difference, then
	a. the rate/number of mode switches AFTER the assistance will be less than without assistance. %Can possibly present windowed approach and show a drop in the number of mode switches...And correlate the "change" to the most informative modes? It is possible that the user identifies the most informative modes on their own in which case the change will be observed. 
	b. the confidence associated with the most likely goal with sharply rise?. This will support our claim that "it will help the robot to make the intent inference confidently, quickly and accurately. "

Tasks were lumped together for previous study. That might still work. What we need to show is the improvement over the course of a trial in which the assistance is requested. 

Ensure that the trials are balanced that is they reach for all goals in a balanced fashion. Enforce only one goal. Don't change the destination. 
Training period..
References, no help than wrong help. HRI - Look into this. 


GENRAL STUFF:

1. Make sure IRB is fine. Questionnaire needs to be revised if needed and then submitted. 
2. Look into payment info. Ask Mahdieh. ClinCard. Email Kathleen with study info and 


%%%%%%%%%%%%%%%%%%%%%%%%
TODO:

1. Implement forward projection of probability distribution. Think of software structure. May need another package for it. Should this be interfacing with blend node at all?
2. Need robot kinematics in Cartesian space. Understanding quaternion math. Need this for forward projection. Refer to Matlab code. Robotic Manipulation. The true robot model is different in different parts of the state space. Should I necessarily bother? Maybe take care of edges of the workspace. Simplistic approach would be treat it as a point mass. and assume infinite state space.  
3. Generate p, pp, ppp etc by projecting for different time lengths. Maybe 1, 2, 3 seconds. 
4. Make sure you understand how the probability distributions are organized. 


DIM1
[p n pp nn] - g1
|p n pp nn] - g2
|p n pp nn] - g3
[p n pp nn] - g4
DIM2
[]
|
|
|
[]
.
.
DIM6
[]
|
|
|
[]
Note: ppp and nnn is never used. So no need for it. Speeds up computation in a small way. 

Each column from each dimension chunk is the projected p distribution. 

5. Tune parameters of DFT to ensure proper intent inference. 

DONE:

1. Implemented DFT intent inference. 
2. 







TIMELINE:

Nov 3rd and 4th:

1. Start writing. 
2. Finish forward projection using DFT module. Don't forget all the undecalred variables in the code (DONE)
3. check if Low pass filtered instantaenous confidence works like DFT oinference. (DONE). Not exactly. DFT is better. 

Nov 5th:

1. Writing. (NOT DONE)


Nov 6th:

1. Finalize coding. (Tuning rise and decay params). 
	Work on blend node. Pick goal with max confidence. Tune arbitration curve (DONE)
2. Brain storm tasks. How to make it difficult? 
3. Create structure on board for paper. 


Nov 7th:
1. Email Kathleen about study. Look into consent forms, Questionnaire etc (FIRST THING IN THE MORNING.) (DONE)
2. Think hard about study design and finalize it. Think about training phase (DID)

Should I have a rqt configure to tune the arbit functions. 
2b. Once study design is kind of finalized, think about trial order, balancing trials, latin square generation etc. Use old scripts if necessary. The trial sheets are in the cabinet. (STudied old scripts and trial order gen). DONE
4. Finish paper structure. Collect references (NOT YET). SOrt of
5. Finalize code tuning. Also play with object setup for tasks. (SOME THOUGHTS)
6. Implement head array code (9:30-11:30). Pull kinova driver button.py update from mahdieh and put it in mico_base ros-kinetic  DONE
EaShould the DFT params be in dynamic areconfigure? MAybe. 


Code related modifications: 
Add distance to the excitatory part of the DS (DONE). Discard angles greater than 90deg? (Probably not. )
 Play around with normalization of the inputs or not? What happens when it reaches the goal? it never gets inside the loop. And it keeps the probability at 1/n. Maybe make it 0?
There is some funny quaternion stuff going on at the edges (top edge of worksapce etc)
Modify translation dynamics to capture limitations at the edges of the workspace. Should I add nooise?

Nov 8th:

1. Print poster (LAST THING DURING THE DAY) (DONE)
2. Test code on Hardware. Code up new object positions and orientation once study is finalized (TESTED A FEW DIFFERENT THINGS)
3. Finalize writing structure. Start putting some dummy content (STARTED). 
Think about figures. Need to finalize all explanatory figure this weekend. (SHared control architecture. )
3b. Write out equations in paper. Notation. indices etc (TO DO) - PARTIALLY
4. Decide on what topics need to recorded. Add headers if needed. Use new msg types if needed. (Data collection pipeline) (TODO)
5. Play with blending params. Pfield speeds. Fix what happens to the probabilities when a goal is reached (KINDA)


Nov 9th. 

1. Try out goal configurations such that the "positions " of two goals are the same, however the orientations are different. Would it disambiguate properly. 

Test out close by locations. The the probability won't rise beyond. TESTED THIS. DID FIXES IN MATLAB. UNNORMALIZED INPUT TO DS. POSSIBLY ELIMINATE GREATE THAN 90 DEGREE INPUT. 

For close by locations the probabilities might not rise at all beyond min threshold. 

1b. Test busy goal configurations. 
 c. Test stacked goal configurations. 
2. What happens to quaternion dynamics when it gets flipped. 
Ideas for figures:


Nov 10th:

1. Look into related works properly. Start writing related works. Make progress of math formalism. start making figures for the intro etc. Think about how to present the motivation figure f or disambiguation. 

2. Make edits to ros code, based on matlab code. If distance > D and angle greater than 90deg: ignore the activation input. 
What is the analogous condition in rotation space. 

Nov 11th. 

1. Start writing experiment section - NOT YET

Nov 12th. Write algorithm - DONE

nov 13th.

1. Practice talk. Read a little bit. Have some general ideas. Prepare for discussion - DONE
2. Work on images in inskscape - STARTED. 

1. Disamb - done
2. Shared control - DONE
3. DFT excitation. - 
figure to motivate disambiguation. Is this a proper simulation. 
4. figure for pribabilities


nov 15th:

1. Update stuff in simulation. Normalization. Some parameter tuning. Gretaer than 90deg. Make sure current input stays 0. add real min properly (DONE) 
2. Think about the exact tasks. NO GRIPPING. BUT STILL MULTISTAGE TASK. 
	TEST HEADRRAY AND REQUEST OF ASSISTANCE WITH HEAD ARRAY, BUTTON SYSTEM (DONE)
	TEST OUT GOAL SCENARIO WITH VERY SIMILAR POSITIONS BY DIFFERENT ORIENTATION. THIS CAN ARISE IN THESE MULTISTAGE TASKS. IN SIMULATION (DONE)
	THINK ABOUT THE TASK. 
	Both tasks are multistage. But first task will have one degree of rotations, similar ones always. Test head array and button for assistance request. 
	How to make sure each segment is DONE? Proximity? Multistage will have issues of 
	What data to collect? How to make analysis easier that way?

	Modify distance such that only if angle is less than 90*. Maybe modify current input with that as well. Did this in MATLAB (DONE)

	Test corner cases where forward projection of rotation is breaking down. What can be done? How to set random modes. How to set best mode. Don't mess this pu.  Use dynamic pfields? 

	Tune blending parameters so that the "quickly" aspect stands out?! 0.55, 0.6? (DONE, ALMOST)

	Can we compute Dk if the p(g), xr is known at any time? Can this be done in matlab. Posthoc. write code in matlab that would do this. with range limits in simulation etc. When a user switches to a mode. We might want to compute what the algorithm might have chosen. 

	rEFERENCES for motion patterns conveying intention. no assistance > incorrect assistance


3. Get ClinCards (DONE)
4. After task is decided. Work on trial generation. 


Nov 16th:

1. Test building up task. 3 positions with 2 stages is 6 different "reaching goals". P
	PLAY WITH BLEND PARAMS. THE MORE NUMBER OF GOALS, THE THRESHOLD HAS TO BE LOWER, BECAUSE INHERENTLY ISOLATION OF GOALS IS HARDER. THE DECAY PARAMETER IN THE DFT HAS TO BE LITTLE MORE? TOO MUCH HYSTERESIS. THE DECAY IN CONFIDENCE HAPPENS WHEN THERE IS NO USER CONTROL COMMAND. THE USER MIGHT BE PLANNING, MIGHT BE SWITCHING MODES, MIGHT BE JUST LOST. 
	THINK ABOUT DISTANCE. IF SOMEONE CAME REALLY CLOSE AND THEN STARTED TO ROTATE SHOULD THE DISTANCE PLAY A ROLE AS WELL. 


TRAINING TASK. 
	1. USE TELEOPERATION. GET FAMILIAR WITH THE DEVICE AND INTERFACE. HAVE THE BLENDING RUNNIG, BUT KEEP ALPHA 0. 
	2. DEMONSTRATE ASSISTANCE LEVEL. TWO GOALS. AND DEMONSTRATE THAT WHEN THE ROBOT IS VERY VERY CONFIDENT IT WILL HELP IN ACHIEVEING THE TASK. 
	3. YOU CAN MAKE THE ROBOT TO STEP IN FASTER AND QUICKER BY PRESSING THIS BUTTON. THE ROBOT WILL SWITCH THE MODE AND YOU CAN CONTINUE TO OPERATE IN IT. 


Nov 17th:

1. implement gripping. Reduce unceritainities in gripping by we doing the gripping. Wizard of Oz (11am)
2. Finish the second task setup. Think of combination. How will making the second phase more complex affect the results. Without assistance quicker. 
In second task, more mode switches will happen. Can be related to task difficulty. 

3. Set up data recording? 

Identify EVERYTHING that is of use. Setup publishers. The timings can be dealt with later. Publish. Have a data recording bash script which will take the filename as argument

4. Reduce teleop speeds. For safety. 
5. Create trial order for 12 subjects (3pm till 6pm)

5. Collect dummy data? 


TESTING TASK:

	DEFINE HOME POSITIONS WITH DIFFERENT LEVELS OF COMPLEXITY. HOW TO ASSESS COMPLEXITY. DISTANCE FROM THE TARGETS + 'DIFFERENCE IN OREINTATION (METRICS EXISTS'. wRITE A NODE THAT CAN COMPUTE THIS? QUICK HACK. GRAB TF SPIT OUT THE METRIC. IDENTIFY POSITIONS WHERE THIS BECOMES LARGER AND LARGER. 


	TASK ONE:
	3 PRESEPCIFED LOCATIONS. STAGE TWO IS ALWAYS EASIER? BECAUSE NO CHANGE IN ORIENTATION. (SORT OF)

	TEST BEST MODE AT EACH OF THE LOCATIONS FOR UNIFORM DISTRIBUTION. (READY)
	TEST BEST MODE AT EACH OF THE LOCATIONS AFTER REACHING THE POSITIONS (THE PROB ASSOCIATED WITH LOCATION WILL BE HIGHER)

	TONIGHT: DEFINE HOME POSITIONS. Some high positions have issues going to some places. Make sure it is fine. 4 home positions is good. DONE

	1 2 3 locations:
	12 23 13 31 32 21 (DOOE)

	1 2 3 4 1D rotation, 2D rotatuion?

	CHANGE rOTATION BY 0.04 speed. Projection. 


Nov 19th:

1. check SD card. 

1. Check if blending gets loaded properly. Load the server AFTER the init values in the node. 
2. Publish potential field vel. Should not be hard. 
3. Play with the memory parameter. Have a stronger memory. And be resistant to sudden changes more. But higher blend value, instead. So it takes time to be confident, but when confident it kicks in hard. 

4. Make separator markers on the sheet. Add serial number by hand. Add pouring task mappings on both pages. Write out launch file switches. 
5. Try out a different trans_disamb file? Add another goal?


TONIGHT:

Email Theo, Luis and Jannie. 

Nov 20th:

1. Continue writing about implementation. Experiments. This should be straightforward if I am smart. 
2. Start to consolidate the data. Run scripts to convert into mat file.s Need to be ready before I leave for Jersey. 
3. Start writing MAtlab code for simulation of best mode at every point in worksapce. Can be done in MATLAB. just right out the quat dynamics properly. 
4. Clear videos. 
TASK TWO:
3 PRESPECIFIED POSITIONS BY VARYING ORIENTATIONS AT EACH POSITION. 
2. TOPICS TO RECORD. NAMING CONVENTION OF FILES. VIDEO SETUP. SCRIPT FOR TRAINING PHASE. 

3. CLINCARD REGISTRATION TALK TO PEOPLE. CLINCARD PDFS TO PEOPLE _DONE


Nov 21:

1. Prepare Consent form, trial list, and questionnaire for H6 today. Setup camera batteries/sd card. 
2. Review experiment script once again. 

Nov 22:

1. Do both studies. 
2. Begin transfer of videos into computer to clear up SD card. Tomorrow morning. 
3. Finish simulation code. pull code into my laptop - PULLED CODE
4. Discuss with Brenna what pictures need to be made???  ODNE


Simplify alorithm. Remove + and -. DONE
nov 23:

1. Transfer vidoes. Clear up SD card. 
2. After studies. Convert bag files into mat. Take pictures of the scene. DONE




ANALYSIS:

1. Pull all data from bag files into appropriate mat sturcture. 

2. Create data structure based on task and interface. This will require code coordination between the bagfile data and trial order data. Use info in trial order data to LOOK up bagfile data and collect them. 


What needs to be collected? COrrelation ebtween times of assistance request and the subsequent confidence.
blue, red, yellow, purple. (1,2,3,4)


CODE STUFF:

1. Write ps3 gripping service. Open and close. 
2. Write trial generation script
3. Write bash script for data collection

ANalysis:

should I convert to matfiles before. Might need original bagfiles. 
Within subject: pre and post switch. 
Across subjects: Number of mode switches. 

*********************************************************************************
*********************************************************************************
*********************************************************************************
*********************************************************************************
*********************************************************************************
*********************************************************************************
*********************************************************************************
*********************************************************************************
*********************************************************************************
*********************************************************************************
*********************************************************************************


PAPER STRUCTURE:

Overarching story:

What is the paper about?
Introduces a formalism for intent inference and an algorithm to maximize intent disambiguation in the context of assistive robotics. 
why is this important?
The success of an assistive system relies heavily on how accurate and confident is the robot in its prediction of human intent. The higher the confidence  the assistance provided will be more accurate and therefore more helpful and useful to the user. 

The disambiguation layer is stacked on top of a given intent inference system and is only as good as the intent inference mechanism. Intent inference schemes which only focuses on instantaneous features discard information from the past human actions and can therefore lose information regarding the underlying human intent. Bayesian approaches work great, however requires a notion of what the human policy is given a task. Further, Bayesian inference under higher order Markovian assumptions can become computationally expensive and can create numerical underflows.  
We devise an intent inference mechanism that specifies how the probability distribution over goals evolves in time deriving inspiration from the dynamic neural field literture. Cite Schoner work. Inference should be sensitive to the appropriate features. Directedness, Distance and Agreement. Directedness is the most informaive one. (Cite intention in motion tyope papers)

Shared control:

Look for work that cites that no help is better than incorrect help. 
Ways to avoid incorrect help. Be super confident about the inference. 
Look at Anca's work to look at arbitration functions that has high thresholds. 

Blending formalism. 

ABSTRACT:

Primary contribution: Intent disambiguation scheme
Secondary contribution: Intent inference mechanism inspired by dynamica field thoery. Why is this novel intent inference scheme necessary. Previous study indictaed that the robustness of the disambiguation scheme is closely tied to the inference power of the intent inference mechanism. Also identified that inference schemes that are not able to incorporate influences of past actions fail to pick contorl modes that are usueful and saisifies the use. 
INTRODUCTION AND MOTIVATION:

Assistive robots are becoming more complex and higher dimensional. However control interfaces used for control and limited and low dimensional. As a result the control space is partitioned into control modes. Shared autonomy can help in reducing the cognitive and physical burden. Talk a little bit about what kinds of shared autonomous systems exist

In order for Shared autonomy to help, the robot should have a method to perform intent inference. The more accurate intent inference, ensure more appropriate assistance which will have higher user acceptance and utility. Intent could either be indicated explicitly (by speech, pointing etc) or inferred. For inference, we typically rely on the information contained in human actions and environment. We focus on inferring intent from the control commands issued via the control interface. Therefore we develop an intent inference scheme. Inference in this context focuses on which goal is the user's intended goal. Maintain probability distribution. The evolution of probability distribution over time is influenced by current and past actions by the human. Biometric signals could give extra cues. but adds to the burden. cumbersome etc. we focus on user control commands. 

However, Due to the sparsity of the signals issued by the user intent inference can be hard for the robot. Infent revealing actions need to be elicited. Elicit human actions that are legible for the robot such that the intent is revealed more clearly. Choose control modes that will reveal the human's intent to the robot more clearly. humans helping robots helping humans. Information seeking actions. 

The proposed disambiguation scheme sits atop the intent inference layer in a shared control paradigm. The sucess of the disambiguation paraidgm is closely tied to the accruacy of the intent inference mechanism. Inference paradigms that are only sensitive to instantaenous features discard past history of control actions and trajectories. Dynamic Bayes networks can be used to perform intent inference. However, the inference becomes harder as the number of time steps increase. In our work, in order to ensure maximal suces of disambiguation siutaion we propose a dynamic field theory inspired mechanism for intent inference. We use principles inspired by dynamic neural fields to specify how the probability distribution over goals should evolve and influence each other. Parameters for rise and decay rate. etc....



RELATED WORK:

References in introduction for control modes, etc. Fix reference. 
SHARED AUTONOMY - ? In general. 

INTENT INFERENCE METHODOLOGIES - Instantaneous features, Bayesian, POMDP, DFT..
MODE SWITCHING ASSISTANCE - Laura's work, Our work. 
INTENT ELICITATION: INFORMATION ACQUISITON - Anca's, Dorsa, Todd's, Atanasov, 
SYNERGY. HELP ME HELP YOU - CMU people. Shared intentionality, human-human. 


Abstract:

MATHEMATICAL FRAMEWORK:

DISAMBIGUATION MECHANISM: Should this be revised? Clarify notation here? Should this be presented before the intent inference mechanism. Maybe make it general enough to convey that this works with any mechanism that specifies the generation of the probability distribution over goals. 
Should recast spatial gradients in terms of time derivative. Is it equivalent to second derivtaive? not exactly...is it>


INTENT INFERENCE MECHANISM:
Experiment Design:
DEscribe the tasks. DEscribe the training phase. DEscribe what each trial was like and the paradigms. 



EXPERIMENTAL DESIGN: STUDY METHODS;

training phase: Subjects gets used to the interfaces. Mostly teleoperation and make them realize how the robot will step in and provide blending assistance when it is super confident. Make sure the blending assistance is not confused with mode switched assistance. 

testing phase: two tasks. Describe the self initiated paradigms

Metrics: Ratio of number of mode switches before and after the trials. How to compare with and without? 
Confidence rise. 
Progress towrads goal. In terms of distance and closeness to target orientation. ??
Total number of mode switches. Rate of mode switches. 
Total time: 



NSF KEYNOTE :MISSY CUMMINGS:

DENIS EDOGUMS

Sesnor fusion. Cognitive System Laborarotu. 





Introduction and Motivation: 
More detailed than RSS. Clarify the structure of the system. Emphasize that this is built on "TOP" of a given intent inference scheme. Figure for the same. 
Related work:
Cite information gathering actions work from Dorsa etc. Info theoretic approaches to information acquisition. 

Intent inference and shared control:
DFT based intent inference scheme. Shared control paradigm. Arbitration that is very selective. For the most part stas out of the way. 
Discuss the intent inference scheme in detail: 
(Perform simulations right away. For instantaneous, and DFt based (incorporates history), maybe distance based as well)

Disambiguation Metric Design:

For DFt based, assume forward projection? Is this like MPC. Stochastic Linear model for robot? Or more sophistaicated nerual net based model? 

Experiment design:

null H1: After the optimal control mode has been selected, the rate of mode switches would not decrease. 
null H2: After the optimal contol mode has been selected, the confidence associated with the most likely goal, is sharply going to increase. 

Results:
statistical analysis tools. what is the nature of data, what kind of significance testing do we need? Time series analysis? Correlation analysis of mode switches with some metric that captures the difficulty of the task?

Discussion: (this should be extended)

Conclusions:


WHAT TO TAKE CARE OF WHEN WRITING THE PAPER:

USE THE BOARD TO VISUALLY STRUCTURE THE PAPER
SECTIONS, SUBSECTIONS, DIAGRAMS, MATH ETC. 

1. Concise clear statements. 
2. Consistency and Simplicity of notation.
3. Check tense and spellings. 
4. Take a fractal approach. Maybe take a look at Konrad's paper writing paper. 





***********************************************
***********************************************
***********************************************
***********************************************
***********************************************
***********************************************

STUDY DESIGN:

Trial Order Scripts are kind of ready: Balanced across all subjects. 

H1: Upon switching to the best disambiguating mode, the rate/number of mode switches will decrease
H2: Upon switching to the best disambiguating mode, robot's confidence in the intended goal will rise sharply.
H3: Upon switching to the best disambiguating mode, the progress (in terms of distance and angle) towards the goal will be faster. 

INTERFACES: J2 and HA. 
TASKS: ONE TRAINING TASK, TWO TASKS
	Should it be single stage tasks or multistage tasks? Multistage tasks take longer. Each stage should be considered a trial. Furthermore, why should we incorporate gripper into the picture. Uncertainities in gripping can get in way of the precise questions we are trying to answer. Our formalism does not consider gripping. It is about getting to a goal location after which further manipulation can be performed. Finger is getting sticky and unpredictable. Don't have time to deal with that. 

TESTING TASK 1:
	1. ALL SAME ORIENTATIONS: 4-5 goals. 
	2. Different orientations. Stacked configurations. 

PARADIGMS: TWO
	1. No help offered for the robot by the human. Full teleoperation. Blending kicks in only when confident. Can look at at what percentage of the time taken for task does the confidence rise happens. If we can show that with the help it happens earlier then, proves over point. 

	2. At least one help offered by the human. 
Training phase:
	Objectives: Getting comfortable with the interfaces and mode switching. 
				Autionomy. 1 objec. 
				Multiple objects. 
				Making them realize that if the robot is really confident then it will step and help in finishing the reaching task. 


				Making them realize that they can help the robot out to become more confident. When ready they can offer help by pressing a button or asking me to convey it to the robot. The robot then will place the control in particular control mode and then user can contoinue to teleoperate the robot. 



Multistage - Without grasping. Define combination. 

Have MATLAB Simulation of the Dk computation. To motivate the disambiguation metric. 
